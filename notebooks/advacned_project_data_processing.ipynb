{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## NORC County Analysis and Pattern-Based Clustering\n",
        "\n",
        "This notebook implements a comprehensive data processing pipeline for analyzing county-level socioeconomic and health indicators from the NORC (National Opinion Research Center) dataset, combined with patterns generated from the AK Analyst Platform.\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "The pipeline processes county-level data through several stages:\n",
        "1. Feature Dictionary Creation\n",
        "2. Pattern Constraint Processing\n",
        "3. County-Pattern Matching\n",
        "4. Data Normalization\n",
        "5. Final Dataset Generation\n",
        "\n",
        "## Data Sources\n",
        "\n",
        "- **NORC Dataset (2017-2020)**: Contains county-level socioeconomic and health indicators including:\n",
        "  - Population demographics\n",
        "  - Education levels\n",
        "  - Economic indicators\n",
        "  - Health facility access\n",
        "  - Social resilience scores\n",
        "  - Death rates\n",
        "\n",
        "- **AK Analyst Patterns**: Generated patterns for death rate analysis (2018-2021)\n",
        "  - Pattern constraints\n",
        "  - Feature relationships\n",
        "  - County classifications\n",
        "\n",
        "This notebook demonstrates the initial data processing phase of our analysis pipeline, focusing on preparing the data for subsequent clustering analysis.\n",
        "\n",
        "#### Author : Drishti Singh\n"
      ],
      "metadata": {
        "id": "mvCFsm3x1PMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Import and Setup\n",
        "This section initializes the required libraries and sets up the Google Drive connection for data access."
      ],
      "metadata": {
        "id": "d23c5hS0z2ZO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81xuV0ZGUXGm",
        "outputId": "e191049b-338b-4b29-d0af-37abe14b2f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration Setup\n",
        "Define source and destination paths for data processing. This ensures consistent file handling throughout the pipeline."
      ],
      "metadata": {
        "id": "GQtKvUBX0B8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change file names here\n",
        "source_folder = '/content/drive/MyDrive/AP Work/source/'\n",
        "destination_folder = '/content/drive/MyDrive/AP Work/result-2/'\n",
        "\n",
        "csv_file = source_folder + 'NORC_data_2017-2020.csv'\n",
        "csv_file_pattern = source_folder +  'patterns_for_death-rate-2018-2021.csv'"
      ],
      "metadata": {
        "id": "1DQiBmEVUrT1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Dictionary Creation\n",
        "Create a mapping between features and their IDs. This step is crucial for maintaining consistent feature references throughout the analysis."
      ],
      "metadata": {
        "id": "y-h7kpQN0GLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "destination_file = destination_folder + 'feature_dict.csv'\n",
        "\n",
        "# Check if the directory exists, and create it if not\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(csv_file)\n",
        "\n",
        "# Extract column names except for 'fips' (considering each column name as a feature)\n",
        "feature_names = [col for col in data.columns if col != 'fips']\n",
        "\n",
        "# Create a DataFrame with Feature_ID and Feature_Name\n",
        "feature_dict = pd.DataFrame({\n",
        "    'Feature_ID': [f'f{i+1}' for i in range(len(feature_names))],\n",
        "    'Feature_Name': feature_names\n",
        "})\n",
        "\n",
        "feature_dict.to_csv(destination_file, index=False)\n",
        "\n",
        "print(f\"Feature dictionary saved to {destination_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md40UEvHVOT7",
        "outputId": "33ff4149-2fc4-4b54-8c4f-b407e527fc38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dictionary saved to /content/drive/MyDrive/AP Work/result-2/feature_dict.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pattern Constraint Processing\n",
        "Process the pattern constraints from the AK Analyst Platform output. This step maps patterns to their corresponding feature constraints."
      ],
      "metadata": {
        "id": "3FtcL8r90Ktr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# Load the feature dictionary\n",
        "feature_dict_csv = destination_folder + 'feature_dict.csv'\n",
        "feature_dict = pd.read_csv(feature_dict_csv)\n",
        "\n",
        "dict_features = feature_dict.set_index('Feature_Name')['Feature_ID'].to_dict()\n",
        "\n",
        "# Load the pattern data\n",
        "patterns_df = pd.read_csv(csv_file_pattern)\n",
        "\n",
        "# Initialize an empty list to store rows for the new CSV\n",
        "output_rows = []\n",
        "\n",
        "# Iterate over each row in the patterns DataFrame\n",
        "for index, row in patterns_df.iterrows():\n",
        "\n",
        "    # Parse the description column (which is a string representation of a dictionary)\n",
        "    description_str = row['description']\n",
        "    description_str = description_str.replace('inf', 'float(\"inf\")').replace('-inf', 'float(\"-inf\")')\n",
        "    # print(description_str)\n",
        "    description = eval(description_str)\n",
        "    pattern_id = description['ID']\n",
        "\n",
        "    # Get constraints dictionary\n",
        "    constraints = description['constraints']\n",
        "\n",
        "    # Iterate over each constraint (feature name and its bounds)\n",
        "    for feature_name, bounds in constraints.items():\n",
        "        # Map the feature name to its ID using dict_features\n",
        "        if feature_name in dict_features:\n",
        "            feature_id = dict_features[feature_name]\n",
        "\n",
        "            # Extract upper bound (ub) and lower bound (lb)\n",
        "            ub = bounds.get('ub', None)\n",
        "            lb = bounds.get('lb', None)\n",
        "\n",
        "            # Append a new row with Pattern_id, feature_id, ub, lb\n",
        "            output_rows.append([pattern_id, feature_id, ub, lb])\n",
        "\n",
        "# Create a DataFrame for output rows\n",
        "output_df = pd.DataFrame(output_rows, columns=['Pattern_id', 'Feature_ID', 'UB', 'LB'])\n",
        "\n",
        "# Save to CSV at the specified destination folder\n",
        "destination_folder = '/content/drive/MyDrive/AP Work/result-2/'\n",
        "output_df.to_csv(f'{destination_folder}/pattern_constraints.csv', index=False)"
      ],
      "metadata": {
        "id": "daBwrxq4BXxK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/AP Work/source/'\n",
        "destination_folder = '/content/drive/MyDrive/AP Work/result-2/'\n",
        "\n",
        "norc_data_path = source_folder + 'NORC_data_2017-2020.csv'\n",
        "pattern_constraints_path = '/content/drive/MyDrive/AP Work/result-2/pattern_constraints.csv'\n",
        "feature_dict_path = destination_folder + 'feature_dict.csv'\n",
        "\n",
        "# Load CSV files\n",
        "pattern_constraints = pd.read_csv(pattern_constraints_path)\n",
        "norc_data = pd.read_csv(norc_data_path)\n",
        "feature_dict = pd.read_csv(feature_dict_path)\n",
        "\n",
        "# Create a dictionary to map Feature_ID to Feature_Name\n",
        "feature_map = dict(zip(feature_dict['Feature_ID'], feature_dict['Feature_Name']))\n",
        "\n",
        "# Initialize a list to store rows for the new CSV\n",
        "output_rows = []\n",
        "\n",
        "# Iterate over each pattern_id in pattern_constraints\n",
        "for pattern_id in pattern_constraints['Pattern_id'].unique():\n",
        "    # Get constraints for this pattern_id\n",
        "    pattern_features = pattern_constraints[pattern_constraints['Pattern_id'] == pattern_id]\n",
        "\n",
        "    # Filter counties in NORC data that satisfy all constraints for this pattern\n",
        "    valid_counties = norc_data.copy()\n",
        "\n",
        "    # Iterate over each feature in this pattern's constraints\n",
        "    for _, row in pattern_features.iterrows():\n",
        "        feature_name = feature_map[row['Feature_ID']]  # Map Feature_ID to Feature_Name\n",
        "        lb, ub = row['LB'], row['UB']\n",
        "\n",
        "        # Apply constraints to filter valid counties\n",
        "        if pd.notna(lb):\n",
        "            valid_counties = valid_counties[valid_counties[feature_name] >= lb]\n",
        "        if pd.notna(ub):\n",
        "            valid_counties = valid_counties[valid_counties[feature_name] <= ub]\n",
        "\n",
        "    # Add valid counties with pattern_id to output_rows\n",
        "    for _, county_row in valid_counties.iterrows():\n",
        "        output_row = [pattern_id] + county_row.tolist()  # Add pattern_id followed by county data\n",
        "        output_rows.append(output_row)\n",
        "\n",
        "# Create a DataFrame fo   r the output rows\n",
        "output_columns = ['Pattern_id'] + norc_data.columns.tolist()\n",
        "output_df = pd.DataFrame(output_rows, columns=output_columns)\n",
        "\n",
        "# Save the output DataFrame as a new CSV file\n",
        "output_df.to_csv(f'{destination_folder}/norc_with_pattern.csv', index=False)"
      ],
      "metadata": {
        "id": "1Z5GsbarZq1X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the filtered_norc_with_pattern.csv file\n",
        "filtered_norc_path = destination_folder + '/norc_with_pattern.csv'\n",
        "filtered_norc = pd.read_csv(filtered_norc_path)\n",
        "\n",
        "# Group by 'Pattern_id' and count the number of rows for each pattern_id\n",
        "pattern_counts = filtered_norc['Pattern_id'].value_counts().reset_index()\n",
        "pattern_counts.columns = ['Pattern_id', 'Row_Count']\n",
        "\n",
        "# Calculate the total number of rows\n",
        "total_rows = len(filtered_norc)\n",
        "\n",
        "# Print the result\n",
        "print(pattern_counts)\n",
        "print(f\"Total number of rows: {total_rows}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS9D2-gZbcZG",
        "outputId": "9fdfaa13-d48d-474a-d03c-07b4646cad23"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Pattern_id  Row_Count\n",
            "0           62       2303\n",
            "1           61       2275\n",
            "2           60       2188\n",
            "3           59       2137\n",
            "4           58       2088\n",
            "..         ...        ...\n",
            "58           4         73\n",
            "59           2         67\n",
            "60           3         66\n",
            "61           1         55\n",
            "62           0         36\n",
            "\n",
            "[63 rows x 2 columns]\n",
            "Total number of rows: 33196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### County-Pattern Matching\n",
        "Match counties to their corresponding patterns based on defined constraints. This creates the foundation for our clustering analysis."
      ],
      "metadata": {
        "id": "KH030V310ogJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# File paths\n",
        "source_folder = '/content/drive/MyDrive/AP Work/source/'\n",
        "destination_folder = '/content/drive/MyDrive/AP Work/result-2/'\n",
        "norc_data_path = source_folder + 'NORC_data_2017-2020.csv'\n",
        "filtered_norc_path = destination_folder + 'norc_with_pattern.csv'\n",
        "\n",
        "# Load CSV files\n",
        "filtered_norc = pd.read_csv(filtered_norc_path)\n",
        "norc_data = pd.read_csv(norc_data_path)\n",
        "\n",
        "# Step 1: Handle 'Urbanicity' by converting 'Urban' to 1 and 'Rural' to 0\n",
        "filtered_norc['Urbanicity'] = filtered_norc['Urbanicity'].map({'Urban': 1, 'Rural': 0})\n",
        "\n",
        "# Step 2: Remove rows with missing values (NaN)\n",
        "filtered_norc = filtered_norc.dropna()\n",
        "\n",
        "# Step 3: Select numeric columns only for normalization (exclude 'Pattern_id' and 'fips')\n",
        "numeric_columns = [col for col in filtered_norc.columns if pd.api.types.is_numeric_dtype(filtered_norc[col]) and col not in ['Pattern_id', 'fips']]\n",
        "\n",
        "# Step 4: Min-Max normalization function\n",
        "def min_max_normalize(df, columns):\n",
        "    return df[columns].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\n",
        "# Step 5: Normalize the selected numeric columns\n",
        "filtered_norc[numeric_columns] = min_max_normalize(filtered_norc, numeric_columns)\n",
        "\n",
        "# Step 6: Save the normalized DataFrame to a new CSV file\n",
        "normalized_output_path = f'{destination_folder}/normalized_norc_with_pattern.csv'\n",
        "filtered_norc.to_csv(normalized_output_path, index=False)\n",
        "\n",
        "print(f\"Normalized data saved to {normalized_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1HMjIls3SB1",
        "outputId": "3a4b3efe-dd81-4bcb-8a5a-f56d258533af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-88da6bb12c98>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  filtered_norc[numeric_columns] = min_max_normalize(filtered_norc, numeric_columns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized data saved to /content/drive/MyDrive/AP Work/result-2//normalized_norc_with_pattern.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Normalization\n",
        "Normalize the dataset to ensure consistent feature scaling and handle categorical variables appropriately"
      ],
      "metadata": {
        "id": "crj6a5qx0lL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def modify_data_based_on_pattern(filtered_norc_path, pattern_constraints_path, destination_folder, feature_dict_path):\n",
        "    # Load the normalized filtered NORC data\n",
        "    filtered_norc = pd.read_csv(filtered_norc_path)\n",
        "\n",
        "    # Load pattern constraints that map Pattern_id to Feature_ID\n",
        "    pattern_constraints = pd.read_csv(pattern_constraints_path)\n",
        "\n",
        "    # Load the feature dictionary to map Feature_ID to Feature_Name\n",
        "    feature_dict = pd.read_csv(feature_dict_path)\n",
        "    feature_map = dict(zip(feature_dict['Feature_ID'], feature_dict['Feature_Name']))\n",
        "\n",
        "    # Create a mapping of Pattern_id to the corresponding feature names\n",
        "    pattern_feature_map = {}\n",
        "    for _, row in pattern_constraints.iterrows():\n",
        "        pattern_id = row['Pattern_id']\n",
        "        feature_id = row['Feature_ID']\n",
        "        feature_name = feature_map.get(feature_id, None)  # Get the feature name from Feature_ID\n",
        "        if feature_name:  # Ensure that the feature name is valid\n",
        "            if pattern_id not in pattern_feature_map:\n",
        "                pattern_feature_map[pattern_id] = []\n",
        "            pattern_feature_map[pattern_id].append(feature_name)\n",
        "\n",
        "    # Step 1: Iterate over the rows of the filtered_norc dataset\n",
        "    output_rows = []\n",
        "\n",
        "    for _, row in filtered_norc.iterrows():\n",
        "        pattern_id = row['Pattern_id']\n",
        "\n",
        "        # Get the features associated with this pattern_id\n",
        "        valid_features = pattern_feature_map.get(pattern_id, [])\n",
        "\n",
        "        # Step 2: Create a new row where features not part of the valid features are set to -1\n",
        "        new_row = row.copy()\n",
        "\n",
        "        # For each column, set it to -1 if the feature isn't part of the current pattern's features\n",
        "        for col in filtered_norc.columns:\n",
        "            if col != 'Pattern_id' and col != 'fips' and col not in valid_features:\n",
        "                new_row[col] = -1  # Set to -1 for features that are not part of the pattern\n",
        "\n",
        "        output_rows.append(new_row)\n",
        "\n",
        "    # Step 3: Create a new DataFrame from the modified rows\n",
        "    modified_norc_df = pd.DataFrame(output_rows)\n",
        "\n",
        "    # Step 4: Save the modified DataFrame to a new CSV file\n",
        "    modified_norc_path = f'{destination_folder}/final.csv'\n",
        "    modified_norc_df.to_csv(modified_norc_path, index=False)\n",
        "\n",
        "    print(f\"Modified data saved to {modified_norc_path}\")\n"
      ],
      "metadata": {
        "id": "dMicJ6YL4O_m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Dataset Generation\n",
        "Generate the final dataset by applying pattern-specific modifications and preparing the data for clustering analysis."
      ],
      "metadata": {
        "id": "AqpcXZwY0hKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_norc_path = '/content/drive/MyDrive/AP Work/result-2/normalized_norc_with_pattern.csv'\n",
        "pattern_constraints_path = '/content/drive/MyDrive/AP Work/result-2/pattern_constraints.csv'\n",
        "destination_folder = '/content/drive/MyDrive/AP Work/result-2/'\n",
        "feature_dict_path = '/content/drive/MyDrive/AP Work/result-2/feature_dict.csv'\n",
        "\n",
        "modify_data_based_on_pattern(filtered_norc_path, pattern_constraints_path, destination_folder, feature_dict_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfjkNXtl4TYx",
        "outputId": "ddcd6567-a1cf-46ce-d8db-616ee93b2de3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified data saved to /content/drive/MyDrive/AP Work/result-2//final.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Validation\n",
        "Perform sanity checks on the processed data to ensure data quality and consistency.\n"
      ],
      "metadata": {
        "id": "8Md4oWD50aG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def sanity_check_clustering_data(normalized_norc_path, feature_dict_path):\n",
        "    # Load the normalized filtered NORC data and feature dictionary\n",
        "    normalized_norc = pd.read_csv(normalized_norc_path)\n",
        "    feature_dict = pd.read_csv(feature_dict_path)\n",
        "\n",
        "    # Get the list of feature names (excluding 'Pattern_id', 'fips', 'Urbanicity')\n",
        "    feature_names = [col for col in normalized_norc.columns if col not in ['Pattern_id', 'fips', 'Urbanicity']]\n",
        "\n",
        "    # Get a dictionary mapping feature names to their feature IDs\n",
        "    feature_dict_map = dict(zip(feature_dict['Feature_Name'], feature_dict['Feature_ID']))\n",
        "\n",
        "    # Initialize a list to store errors\n",
        "    errors = []\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = normalized_norc[normalized_norc.isnull().any(axis=1)]\n",
        "    if not missing_values.empty:\n",
        "        for _, row in missing_values.iterrows():\n",
        "            errors.append((row['Pattern_id'], row['fips'], 'Missing value in row'))\n",
        "\n",
        "    # Iterate through each row to check for issues\n",
        "    for index, row in normalized_norc.iterrows():\n",
        "        pattern_id = row['Pattern_id']\n",
        "        fips = row['fips']\n",
        "\n",
        "        # Check each feature (column) to see if it is normalized correctly (i.e., between 0 and 1)\n",
        "        for feature_name in feature_names:\n",
        "            # If the feature does not belong to the current pattern, check if its value is -1\n",
        "            if pd.isna(row[feature_name]) or row[feature_name] < 0 or row[feature_name] > 1:\n",
        "                if row[feature_name] != -1:\n",
        "                    errors.append((pattern_id, fips, feature_name, row[feature_name]))\n",
        "\n",
        "        # Check for duplicate columns in the dataset\n",
        "        duplicate_columns = normalized_norc.columns[normalized_norc.columns.duplicated()]\n",
        "        if not duplicate_columns.empty:\n",
        "            errors.append((pattern_id, fips, 'Duplicate column', list(duplicate_columns)))\n",
        "\n",
        "    # Print the errors if found\n",
        "    if errors:\n",
        "        print(\"Issues found in the following rows:\")\n",
        "        for error in errors:\n",
        "            print(f\"Pattern ID: {error[0]}, County ID (fips): {error[1]}, Feature: {error[2]}, Value: {error[3]}\")\n",
        "    else:\n",
        "        print(\"Sanity check passed. No issues found.\")\n",
        "\n",
        "# Path to the normalized filtered NORC file and feature dictionary\n",
        "normalized_norc_path = '/content/drive/MyDrive/AP Work/result-2/final.csv'\n",
        "feature_dict_path = '/content/drive/MyDrive/AP Work/result-2/feature_dict.csv'\n",
        "\n",
        "# Perform the sanity check for clustering\n",
        "sanity_check_clustering_data(normalized_norc_path, feature_dict_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwn73Iwo74F_",
        "outputId": "8bf2cf89-71cf-484c-eb21-84674011de1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity check passed. No issues found.\n"
          ]
        }
      ]
    }
  ]
}